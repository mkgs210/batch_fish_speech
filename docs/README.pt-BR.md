[English](../README.md) | [Russian](README.ru.md) | [ç®€ä½“ä¸­æ–‡](README.zh.md) | **Portuguese** | [æ—¥æœ¬èª](README.ja.md) | [í•œêµ­ì–´](README.ko.md)<br>

# Fish Speech Batch Inference

Este Ã© um fork do **Fish Speech** com suporte aprimorado para inferÃªncia em batch, para geraÃ§Ã£o eficiente de voz.

## ğŸš€ Recursos

- **Processamento em lote**: Processa mÃºltiplos textos simultaneamente para acelerar a inferÃªncia  
- **EstÃ¡vel e eficiente**: Sem resultados vazios, com cÃ¡lculos otimizados e mÃ¡scaras de atenÃ§Ã£o corretas

## ğŸ› ï¸ Uso

1. **Baixe o modelo do codec**.  
2. **Crie o arquivo `fake.npy` com seu Ã¡udio de referÃªncia e o caminho para o checkpoint**:

    ```bash
    python fish_speech/models/dac/inference.py \
        -i "ref_audio_name.wav" \
        --checkpoint-path "checkpoints/openaudio-s1-mini/codec.pth"
    ```

    Esse comando irÃ¡ gerar o arquivo `fake.npy` (Ã© possÃ­vel especificar caminho de saÃ­da).

3. **Configure o caminho para o `fake.npy` no arquivo `fish_batch_inference.py`.**

4. **Execute a inferÃªncia em batch**:

    ```bash
    python fish_batch_inference.py
    ```

## ğŸ”„ Roteiro

- **ParalelizaÃ§Ã£o do VQ-GAN** para inferÃªncia ainda mais rÃ¡pida  
- **Interface Web Gradio** para facilitar o processamento em lote

## ğŸ“Š Desempenho

- **Velocidade**: AtÃ© 3-4 vezes mais rÃ¡pido que o processamento sequencial  
- **Qualidade**: Resultados em Ã¡udio mais diversos e robustos

**RepositÃ³rio**: https://github.com/mkgs210/batch_fish_speech

*Fork do Fish Speech com inferÃªncia em batch completa. Suporte a VQ-GAN paralelo e Gradio estÃ¡ a caminho!*

[1] https://github.com/mkgs210/batch_fish_speech/blob/main/docs/docs/README.pt-BR.md
